---
layout:     post
title:      "WORK IN PROGRESS: 2nd Place Solution to 2017 NDSB"
date:       2017-04-18 00:00:00
author:     "Daniel Hammack"
---


## Foreward

This blog post describes the *story* behind my contribution to the 2nd place solution to the [2017 Data Science Bowl](https://www.kaggle.com/c/data-science-bowl-2017/). I will try to describe here *why and when* I did certain things but avoid the deep details on exactly how everything works. For those details see my [technical report](https://github.com/dhammack/dhammack.github.io/2017dsbreport.pdf) which has more of an academic flavor. I'll try to go roughly in chronological order here. 

## The Results
I hate it when the final result is used to maintain suspense throughout an article. So here are the results up front:

**2nd place finish in the largest Kaggle competition to date** (in terms of total prize pool = $1 million)

And here's two cool .gifs showing one of my models at work (red = bad):

<center>
<p><img src="/images/global_importance_db8e5fe2c0c7e92db6cac98df51c3802.gif" alt="most important parts of the scan" style="width:400px;height:400px;"/>
<img src="/images/global_importance_f31e47d431624a0ea59b84de044ae55e.gif" alt="most important parts of another scan" style="width:400px;height:400px;"/>
</p>
</center>

## The Beginning

I got an email when the 2017 DSB launched. It said something along the lines of "3D images and a million bucks" and I was sold. I haven't worked on 3D images before this so I thought it would be a good learning experience. The fact that there were payouts for the top 10 finishers was also quite motivating.

#### Preprocessing

The beginning of the competition was focused on data prep. As I had no prior background with DICOM files, I had to figure out how to get the data into a format that I was familiar with - numpy arrays.

This turned out to be fairly straightforward, and the preprocessing code that I wrote on the second day of the competition I continued using until the very end. After browsing the forum, reading about CT scans, and reading some of the reports from the [LUNA16 challenge](https://luna16.grand-challenge.org/) I was good to go. 

Basically CT scans are a collection of 2D greyscale slices (regular images). So you just need to concatenate them all in the right order (and scale them using the specified slope + intercept) and you're set. A tricky detail that I found reading the LUNA competition is that different CT machines will produce scans with different sampling rates in the 3rd dimension. The distance between the consecutive images is called the 'slice thickness' and can vary up to 4x between scans. So in order to apply the same model to scans of different thickness (and to make a model generalize to new scans) you need to resize the images so that they have the same resolution. All of the solutions I've seen including my own sampled the scans to 1 mm^3 per voxel (volumetric pixel).

So the first thing I did was convert all the DICOM data into normalized 3D numpy arrays.

#### External Data

Keeping an eye on the [external data thread](https://www.kaggle.com/c/data-science-bowl-2017/discussion/27666) post on the Kaggle forum, I noticed that the LUNA dataset looked very promising and downloaded it at the beginning of the competition. 

The LUNA16 challenge is a computer vision challenge essentially with the goal of finding 'nodules' in CT scans. It contains about 900 additional CT scans. In case you are not familiar, a 'nodule' is another word for a tumor. Here's an example of a malignant nodule (highlighted in blue):

<center>
<p><img src="/images/most_malignant_nodule_db8e5fe2c0c7e92db6cac98df51c3802.gif" alt="malignant nodule" style="width:512px;height:512px;"/>
</p>
</center>

This is from a small 3D chunk of a full scan. To put this nodule in context, look at the first big .gif in this post.

Anyway, the LUNA16 dataset had some very crucial information - the locations in the LUNA CT scans of 1200 nodules. See, finding nodules in a CT scan is hard (for a computer). Very hard. An average CT scan is 30 x 30 x 40 centimeters cubed while an average nodule is 1cm cubed. This means that the average scan is 36,000 times larger in volume than the cancer we're looking for. For an automated system with zero knowledge of human anatomy (and actually zero prior knowledge at all), figuring out which one or two areas in a scan really matter is a very hard task. It's like showing someone a 150 page report (single-spaced) and telling them that there is one word misspelled that they need to find.  

So this LUNA data was very important. To sweeten the deal, the LUNA dataset turns out to be a curated subset of a larger dataset called the [LIDC-IDRI](https://wiki.cancerimagingarchive.net/display/Public/LIDC-IDRI) data. Now most of the information in these two datasets is the same, but the LIDC dataset has one thing that LUNA didn't - radiologist descriptions of each nodule they found. The creators of the LUNA dataset threw out this data when they created their dataset (because it wasn't relevant to them). However it is extremely relevant to the task of predicting cancer diagnosis.

So I downloaded the LIDC annotations and joined them onto the LUNA dataset. Ultimately this means that I had 1200 nodules and radiologist estimations of their properties. The properties that I chose to use were:

* nodule malignancy (obviously!)
* nodule diameter (size in mm, bigger is usually more cancerous)
* nodule spiculation (how 'stringy' a nodule is - more is worse)
* nodule lobulation (how 'bubbly' a nodule is - more is worse)

There is a [report](http://www.clevelandclinicmeded.com/medicalpubs/diseasemanagement/hematology-oncology/pulmonary-nodules/) posted on the forums that describes some of the relationships between nodule attributes and malignancy.

Figuring out that the LIDC dataset had malignancy labels turned out to be one of the biggest separators between teams in the top 5 and the top 15. The 7th place team, for example, probably would have placed top 5 if they had [seen that LIDC had malignancy](https://www.kaggle.com/c/data-science-bowl-2017/discussion/31576). 

The way I found the LIDC malignancy information is actually a funny story. A month into the competition, someone made a submission to the stage 1 leaderboard that was insanely good. In hindsight they were [cheating](https://www.kaggle.com/c/data-science-bowl-2017/discussion/27662), but at the time I didn't know this. I assumed they had discovered some great additional source of data so I dug around more and found the LIDC malignancy labels!

## First Approaches ##

The beginning of a competition is the most interesting part, especially when there isn't an obvious solution to the problem like this one. For at least a week I tried a few things without success, namely:

* Downsampling all the data to 128 x 128 x 128 mm and building 'global' models (NOT using the LUNA data)
* Breaking up the 1400 training scans into smaller chunks and trying to predict whether each chunk belonged to a cancer/noncancer scan

I think the third thing I tried was using the LUNA data. At first I built a model (using 64mm cube chunks) where the model was trained to predict the probability of a given chunk containing a nodule. Then to generate a prediction for a whole scan (remember 300 x 300 x 400 mm in size), I "rolled" my model over the whole scan to get a prediction at each location. To make sure not to miss any parts, the model needs to be scored a few hundred times.

Doing this gives you a 3D grid of 'nodule probabilities' (because the model predicts the probability of a nodule at each location). I then aggregated these with some simple stats like max, stdev, and the location of the max probability prediction.

After doing some initial tests on the training set (cross validation), I was expecting my leaderboard score to be around 0.482 (random guessing will get you 0.575). I did a submission and it scored 0.502 on the stage 1 leaderboard which was a little disappointing. However it was good enough to put me in the top 10 for a few days. And I finally found something that worked!

#### Pulling the Thread ####

Now that I had something that worked, I decided to see how far I could push it. There were a couple of no-brainer improvements that I made:

* Instead of predicting probability of a nodule existing, predict the size of the nodule (which is in the LUNA dataset).
* Add data augmentation (described below)
* Improved model architecture (mainly added more batch norm)
* After discovering their existance, add LIDC features (malignancy especially)
* Improved aggregation of chunk predictions

Doing all this improved my cross-validation score (my estimated leaderboard score) to 0.433! So naturally I did a submission and it came out significantly worse at 0.460. 

#### Data Augmentation ####

What if I told you you could have an **infinite** amount of data to build your models on? Well with Data Augmentationâ„¢ you can! 

Data augmentation is a crucial but subtle part of my solution, and in general is one of the reasons that neural networks are so great for computer vision problems. It's the reason that I am able to build models on only 1200 samples (nodules) and have them work very well (normal computer vision datasets have 10,000 - 10,000,000 images). 

The idea is this - there are certain transformations that you can apply to your data which don't really 'change' it, but they change the way it looks. Look at these two pictures:

<center>
<p><img src="/images/giraffe.jpg" alt="giraffe" style="width:300px;height:600px;"/>
<img src="/images/giraffe_mirror.jpg" alt="effarig" style="width:300px;height:600px;"/>
</p>
</center>

They're both giraffes! However to a neural network model these are totally different inputs. It might think one is a giraffe and another is a lion. 

Mirroring is an example of a 'lossless transformation' of an image. Lossless here is in terms of information - you don't lose any information when you mirror an image. This is opposed to 'lossy transformations' which do throw away some information. A rotation by 10 degrees is an example of a lossy transformation - some of the pixels will get a little messed up but the overall spirit of the image is the same.

With 3D images, there are tons of transformations you can use, both lossy and lossless. This sort of thing is studied in a branch of mathematics called Group Theory, but just using some quick googling we can find out that there are [48 unique lossless permutations of 3D images](https://en.wikipedia.org/wiki/Octahedral_symmetry#The_isometries_of_the_cube) as opposed to only 8 for 2D images! In both 2D and 3D there are an infinite number of lossy transformations as well.

Here's a cool graphic of the lossless permutations of a 2D image:

<center>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/3/3e/Dihedral_group4_example.png/180px-Dihedral_group4_example.png" alt="there are 8 lossless permutations of a 2d image" style="width:300px;height:300px;"/>
</p>
</center>

So how does this help us? Well each time before showing a chunk from a CT scan to the model, the chunk can be transformed so that it's 'meaning' remains the same but it looks different. This teaches our model to ignore the exact way an image is presented and instead focus on the unchanging information contained in the image. We say that the model becomes 'invariant' to the transformations we use which means that applying those transformations will no longer change it's prediction.

However the model, like some people I know, isn't perfect. Sometimes if you rotate an image in a certain way, the model will change its prediction of malignancy a little. To exploit this, you can show an image to the model a bunch of times with different random transformations and average the predictions it gives you. This is called "test time augmentation" and is another trick I used to improve performance.

## The Final Stretch ## 

I spent the remainder of the competition fine tuning this basic approach. A lot of experimentation was done with the objective function for the models, what fraction of nodules/non-nodules to use in training, and the best way to generate a global diagnosis from the nodule predictions. 

I also came up with a neat trick for speeding up the training of my models during this phase. During experimentation, I found that if you build models on 32x32x32 mm crops of nodules they train much faster and achieve much better accuracy. However when you want to apply that model across a full scan, you have to evaluate it something crazy like 3000 times. Each time the model is evaluated at a location there is a chance of a false positive, so more evaluations is definitely not desirable. Building 64x64x64 models, on the other hand, takes longer and isn't quite as good at describing nodules but ultimately works better. Comparing the two, the 64 sized model requires 8x fewer evaluations than the 32 sized model while only being slightly less accurate. 

A reasonable question to ask at this point is - why not bigger than 64? Well I tried that. It turns out that 64 is a sort of 'sweet spot' in chunk size. Remember that our models rely heavily on exploiting the symmetries of 3D space. Well it turns out that the lungs in general aren't that symmetric. So if you keep making your chunk size larger, data augmentation becomes less effective. I do believe that a chunk size of 128 *could* work, but I didn't have the patience to train models of that size as it generally takes 8x longer than 64 sized models.

Anyway, one of the nice things about the architecture that I used was that the model can be trained on any sized input (of at least 32x32x32 in size). This is because the last pooling layer in my model was a **global** max pooling layer which returns a fixed length output no matter the input size. Because of this, I was able to use 'curriculum learning' to speed up model training. 

[Curriculum learning](https://ronan.collobert.com/pub/matos/2009_curriculum_icml.pdf) is a technique in machine learning where a model is first trained on simpler or easier samples before gradually progressing to harder samples. Since the 32x32x32 chunks are easier/faster to train on than 64x64x64, I trained the model on size 32 chunks first and then 64 chunks after. 

