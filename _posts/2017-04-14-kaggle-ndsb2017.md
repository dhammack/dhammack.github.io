---
layout:     post
title:      "WORK IN PROGRESS: 2nd Place Solution to 2017 NDSB"
date:       2017-04-18 00:00:00
author:     "Daniel Hammack"
---


## Foreward

This blog post describes the *story* behind my contribution to the 2nd place solution to the [2017 Data Science Bowl](https://www.kaggle.com/c/data-science-bowl-2017/). I will try to describe here *why and when* I did certain things but avoid the deep details on exactly how everything works. For those details see my [technical report](https://github.com/dhammack/dhammack.github.io/2017dsbreport.pdf) which has more of an academic flavor. I'll try to go roughly in chronological order here. 

## The Results
I hate it when the final result is used to maintain suspense throughout an article. So here are the results up front:

**2nd place finish out of 2000 teams and $1M total prize pool**

And here's two cool .gifs showing one of my models at work (red = bad):

<center>
<p><img src="/images/global_importance_db8e5fe2c0c7e92db6cac98df51c3802.gif" alt="most important parts of the scan" style="width:400px;height:400px;"/>
<img src="/images/global_importance_f31e47d431624a0ea59b84de044ae55e.gif" alt="most important parts of another scan" style="width:400px;height:400px;"/>
</p>
</center>

## The Beginning

I got an email when the 2017 DSB launched. It said something along the lines of "3D images and a million bucks" and I was sold. I haven't worked on 3D images before this so I thought it would be a good learning experience. The fact that there were payouts for the top 10 finishers was also quite motivating.

#### Preprocessing

The beginning of the competition was focused on data prep. As I had no prior background with DICOM files, I had to figure out how to get the data into a format that I was familiar with - numpy arrays.

This turned out to be fairly straightforward, and the preprocessing code that I wrote on the second day of the competition I continued using until the very end. After browsing the forum, reading about CT scans, and reading some of the reports from the [LUNA16 challenge](https://luna16.grand-challenge.org/) I was good to go. 

Basically CT scans are a collection of 2D greyscale slices (regular images). So you just need to concatenate them all in the right order (and scale them using the specified slope + intercept) and you're set. A tricky detail that I found reading the LUNA competition is that different CT machines will produce scans with different sampling rates in the 3rd dimension. The distance between the consecutive images is called the 'slice thickness' and can vary up to 4x between scans. So in order to apply the same model to scans of different thickness (and to make a model generalize to new scans) you need to resize the images so that they have the same resolution. All of the solutions I've seen including my own sampled the scans to 1 mm^3 per voxel (volumetric pixel).

So the first thing I did was convert all the DICOM data into normalized 3D numpy arrays.

#### External Data

Keeping an eye on the [external data thread](https://www.kaggle.com/c/data-science-bowl-2017/discussion/27666) post on the Kaggle forum, I noticed that the LUNA dataset looked very promising and downloaded it at the beginning of the competition. 

The LUNA16 challenge is a computer vision challenge essentially with the goal of finding 'nodules' in CT scans. It contains about 900 additional CT scans. In case you are not familiar, a 'nodule' is another word for a tumor. Here's an example of a malignant nodule (highlighted in blue):

<center>
<p><img src="/images/most_malignant_nodule_db8e5fe2c0c7e92db6cac98df51c3802.gif" alt="malignant nodule" style="width:512px;height:512px;"/>
</p>
</center>

This is from a small 3D chunk of a full scan. To put this nodule in context, look at the first big .gif in this post.

Anyway, the LUNA16 dataset had some very crucial information - the locations in the LUNA CT scans of 1200 nodules. See, finding nodules in a CT scan is hard (for a computer). Very hard. An average CT scan is 30 x 30 x 40 centimeters cubed while an average nodule is 1cm cubed. This means that the average scan is 36,000 times larger in volume than the cancer we're looking for. For an automated system with zero knowledge of human anatomy (and actually zero prior knowledge at all), figuring out which one or two areas in a scan really matter is a very hard task. It's like showing someone a 150 page report (single-spaced) and telling them that there is one word misspelled that they need to find.  

So this LUNA data was very important. To sweeten the deal, the LUNA dataset turns out to be a curated subset of a larger dataset called the [LIDC-IDRI](https://wiki.cancerimagingarchive.net/display/Public/LIDC-IDRI) data. Now most of the information in these two datasets is the same, but the LIDC dataset has one thing that LUNA didn't - radiologist descriptions of each nodule they found. The creators of the LUNA dataset threw out this data when they created their dataset (because it wasn't relevant to them). However it is extremely relevant to the task of predicting cancer diagnosis.

So I downloaded the LIDC annotations and joined them onto the LUNA dataset. Ultimately this means that I had 1200 nodules and radiologist estimations of their properties. The properties that I chose to use were:

* nodule malignancy (obviously!)
* nodule diameter (size in mm, bigger is usually more cancerous)
* nodule spiculation (how 'stringy' a nodule is - more is worse)
* nodule lobulation (how 'bubbly' a nodule is - more is worse)

There is a [report](http://www.clevelandclinicmeded.com/medicalpubs/diseasemanagement/hematology-oncology/pulmonary-nodules/) posted on the forums that describes some of the relationships between nodule attributes and malignancy.

Figuring out that the LIDC dataset had malignancy labels turned out to be one of the biggest separators between teams in the top 5 and the top 15. The 7th place team, for example, probably would have placed top 5 if they had [seen that LIDC had malignancy](https://www.kaggle.com/c/data-science-bowl-2017/discussion/31576). 

The way I found the LIDC malignancy information is actually a funny story. A month into the competition, someone made a submission to the stage 1 leaderboard that was insanely good. In hindsight they were [cheating](https://www.kaggle.com/c/data-science-bowl-2017/discussion/27662), but at the time I didn't know this. I assumed they had discovered some great additional source of data so I dug around more and found the LIDC malignancy labels!

## First Approaches ##

The beginning of a competition is the most interesting part, especially when there isn't an obvious solution to the problem like this one. For at least a week I tried a few things without success, namely:

* Downsampling all the data to 128 x 128 x 128 mm and building 'global' models (NOT using the LUNA data)
* Breaking up the 1400 training scans into smaller chunks and trying to predict whether each chunk belonged to a cancer/noncancer scan

I think the third thing I tried was using the LUNA data. At first I built a model (using 64mm cube chunks) where the model was trained to predict the probability of a given chunk containing a nodule. Then to generate a prediction for a whole scan (remember 300 x 300 x 400 mm in size), I "rolled" my model over the whole scan to get a prediction at each location. To make sure not to miss any parts, the model needs to be scored a few hundred times.


