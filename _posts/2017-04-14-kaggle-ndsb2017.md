---
layout:     post
title:      "WORK IN PROGRESS: 2nd Place Solution to 2017 NDSB"
date:       2017-04-18 00:00:00
author:     "Daniel Hammack"
---


## Foreward

This blog post describes the *story* behind my contribution to the 2nd place solution to the [2017 Data Science Bowl](https://www.kaggle.com/c/data-science-bowl-2017/). I will try to describe here *why and when* I did certain things but avoid the deep details on exactly how everything works. For those details see my [technical report](https://github.com/dhammack/dhammack.github.io/2017dsbreport.pdf) which has more of an academic flavor. I'll try to go roughly in chronological order here. 

## The End
I hate it when the final result is used to maintain suspense throughout an article. So here are the results up front:

* Competition goal: given CT scan, forecast probability of lung cancer diagnosis in next year
* 2nd place finish out of 2000 teams and $1M total prize pool
* Teamed up with Julian de Wit
* Used lots of 3D Convolutional Neural Networks

And here's two cool .gifs showing one of my models at work (red = bad):

<center>
<p><img src="/images/global_importance_db8e5fe2c0c7e92db6cac98df51c3802.gif" alt="most important parts of the scan" style="width:400px;height:400px;"/>
<img src="/images/global_importance_f31e47d431624a0ea59b84de044ae55e.gif" alt="most important parts of another scan" style="width:400px;height:400px;"/>
</p>
</center>

## The Beginning

This is why I competed in the 2017 Data Science Bowl (DSB):

<center>
<p><img src="/images/start_email.PNG" alt="start email" /> </p>
</center>

It wasn't hard to convince me to compete. Just 3D images + big prize pool did it for me. I haven't worked on 3D images before this so I thought it would be a good learning experience. The fact that there were payouts for the top 10 finishers was also quite motivating.

#### Preprocessing

The beginning of the competition was focused on data prep. As I had no prior background with DICOM files, I had to figure out how to get the data into a format that I was familiar with - numpy arrays.

This turned out to be fairly straightforward, and the preprocessing code that I wrote on the second day of the competition I continued using until the very end. After browsing the forum, reading about CT scans, and reading some of the reports from the [LUNA16 challenge](https://luna16.grand-challenge.org/) I was good to go. 

Basically CT scans are a collection of 2D greyscale slices (regular images). So you just need to concatenate them all in the right order (and scale them using the specified slope + intercept) and you're set. A tricky detail that I found reading the LUNA competition is that different CT machines will produce scans with different sampling rates in the 3rd dimension. The distance between the consecutive images is called the 'slice thickness' and can vary up to 4x between scans. So in order to apply the same model to scans of different thickness (and to make a model generalize to new scans) you need to resize the images so that they have the same resolution. All of the solutions I've seen including my own sampled the scans to 1 mm^3 per voxel (volumetric pixel).

So the first thing I did was convert all the DICOM data into normalized 3D numpy arrays.

#### External Data

Keeping an eye on the [external data thread](https://www.kaggle.com/c/data-science-bowl-2017/discussion/27666) post on the Kaggle forum, I noticed that the LUNA dataset looked very promising and downloaded it at the beginning of the competition. 

The LUNA16 challenge is a computer vision challenge essentially with the goal of finding 'nodules' in CT scans. It contains about 900 additional CT scans. In case you are not familiar, a 'nodule' is another word for a tumor. Here's an example of a malignant nodule (highlighted in blue):

<center>
<p><img src="/images/most_malignant_nodule_db8e5fe2c0c7e92db6cac98df51c3802.gif" alt="malignant nodule" style="width:512px;height:512px;"/>
</p>
</center>

This is from a small 3D chunk of a full scan. To put this nodule in context, look at the first big .gif in this post.

Anyway, the LUNA16 dataset had some very crucial information - the locations in the LUNA CT scans of 1200 nodules. See, finding nodules in a CT scan is hard (for a computer). Very hard. An average CT scan is 30 x 30 x 40 centimeters cubed while an average nodule is 1cm cubed. This means that the average scan is 36,000 times larger in volume than the cancer we're looking for. For an automated system with zero knowledge of human anatomy (and actually zero prior knowledge at all), figuring out which one or two areas in a scan really matter is a very hard task. It's like showing someone a 150 page report (single-spaced) and telling them that there is one word misspelled that they need to find.  

So this LUNA data was very important. To sweeten the deal, the LUNA dataset turns out to be a curated subset of a larger dataset called the [LIDC-IDRI](https://wiki.cancerimagingarchive.net/display/Public/LIDC-IDRI) data. Now most of the information in these two datasets is the same, but the LIDC dataset has one thing that LUNA didn't - radiologist descriptions of each nodule they found. The creators of the LUNA dataset threw out this data when they created their dataset (because it wasn't relevant to them). However it is extremely relevant to the task of predicting cancer diagnosis.

So I downloaded the LIDC annotations and joined them onto the LUNA dataset. Ultimately this means that I had 1200 nodules and radiologist estimations of their properties. The properties that I chose to use were:

* nodule malignancy (obviously!)
* nodule diameter (size in mm, bigger is usually more cancerous)
* nodule spiculation (how 'stringy' a nodule is - more is worse)
* nodule lobulation (how 'bubbly' a nodule is - more is worse)

There is a [report](http://www.clevelandclinicmeded.com/medicalpubs/diseasemanagement/hematology-oncology/pulmonary-nodules/) posted on the forums that describes some of the relationships between nodule attributes and malignancy.


